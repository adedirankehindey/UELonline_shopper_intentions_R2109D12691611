# -*- coding: utf-8 -*-
"""yunus_adediran_online_shopper_intention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuKil3FzHeO_YlDU313wUKpxXbUMVWuI

# Predictive Analysis of Customer Visits Data

**Author:** ADEDIRAN, Yunus Kehinde

## Objective
1. The objective of this analysis is to build a predictive model to determine whether a customer will make a purchase on the website based on their visit data.
2. To compare the transforms performance on the predictive algorithm


## Dataset
The dataset contains 12,330 sessions of customer visits to the website.

### Features

**18 attributes** are present in the:
- **10 Numerical Attributes**
- **8 Categorical Attributes**

### Attribute Information
- **Revenue**: Class level. Possible values: **False** (did not buy) and **True** (made a purchase).
- **Administrative**: Number of administrative pages visited during the session.
- **Administrative Duration**: Total time spent on administrative pages in seconds.
- **Informational**: Number of informational pages visited during the session.
- **Informational Duration**: Total time spent on informational pages in seconds.
- **Product Related**: Number of product-related pages visited during the session.
- **Product Related Duration**: Total time spent on product-related pages in seconds.
- **Bounce Rate**: Percentage of visitors who leave the site after viewing only one page.
- **Exit Rate**: Percentage of exits on a specific page.
- **Page Value**: Average value of a web page prior to completing an e-commerce transaction.
- **Special Day**: Indicates the proximity of the visit date to a specific special day.
- **Operating System**: The operating system used by the visitor.
- **Browser**: The browser used to access the website.
- **Region**: Geographic region of the visitor.
- **Traffic Type**: Type of traffic (e.g., direct, referral).
- **Visitor Type**: Indicates whether the visitor is new or returning.
- **Weekend**: Boolean indicating whether the visit occurred on a weekend.
- **Month**: Month of the year during which the visit occurred.
"""

import sys
print(sys.version)

!pip install ucimlrepo

# Commented out IPython magic to ensure Python compatibility.
# Data loading and EDA packages
from ucimlrepo import fetch_ucirepo
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import scipy as sc
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

#statistical and ml model packages
import sklearn as sk
from scipy import stats
from scipy.stats import kruskal, mannwhitneyu, friedmanchisquare, f_oneway, ttest_ind,spearmanr, shapiro, levene
from sklearn.cluster import KMeans
import statsmodels.api as sm

# preprocessing packages
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler, PowerTransformer, QuantileTransformer
from sklearn.model_selection import train_test_split
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.model_selection import RandomizedSearchCV, train_test_split, StratifiedKFold
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# model packages
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression

# model evaluation packages
from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, roc_auc_score

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

"""#### Data Loading fom UCI library"""

# fetch dataset
online_shoppers_purchasing_intention_dataset = fetch_ucirepo(id=468) #pd.read_csv("online_shoppers_intention.csv") #

"""#### Data Inspection and Cleaning"""

X = online_shoppers_purchasing_intention_dataset.data.features
y = online_shoppers_purchasing_intention_dataset.data.targets

df = pd.concat([X, y], axis=1)

df.sample(30)

df_copy = df.copy()

df.sample(10)

df.info()

df_copy['OperatingSystems'] = df_copy['OperatingSystems'].astype('category')
df_copy['Browser'] = df_copy['Browser'].astype('category')
df_copy['Region'] = df_copy['Region'].astype('category')
df_copy['TrafficType'] = df_copy['TrafficType'].astype('category')
df_copy['VisitorType'] = df_copy['VisitorType'].astype('category')
df_copy['Month'] = df_copy['Month'].astype('category')


df_copy.info()

num_cols = df_copy.select_dtypes(include=['number']).columns.tolist()

cat_cols = df_copy.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()

"""##### Descriptive Statistics of Numerical Columns"""

df_copy[num_cols].describe()

"""##### Descriptive Statistics of Categorical Columns"""

df_copy[cat_cols].describe()

missing_values=df.isnull().sum()
missing_values

"""### Exploratory Data Analysis"""

target_name = "Revenue"

df_copy.Revenue.value_counts()

revenue_counts = df['Revenue'].value_counts()
revenue_labels = ['False', 'True']
revenue_sizes = revenue_counts.values
revenue_percentages = [f'{value / len(df) * 100:.1f}%' for value in revenue_sizes]

explode = (0.09, 0.09)

plt.figure(figsize=(6, 6))
plt.pie(
    revenue_sizes,
    labels=revenue_labels,
    autopct='%1.1f%%',
    explode=explode,
    startangle=140,
    shadow=True,
    colors=['#4E79A7', '#F28E2B']
)


plt.title("Distribution of Revenue")
plt.show()

"""#We need encoding before doing the one-SVM
# Splitting the dataset
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)

# Train One-Class SVM on the normal data
oc_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.1)  # nu is the upper bound on the fraction of training errors
oc_svm.fit(X_train)

# Predicting on the test set
y_pred_train = oc_svm.predict(X_train)
y_pred_test = oc_svm.predict(X_test)

# Transform predictions to binary (1 for inliers, -1 for outliers)
y_pred_train = [1 if x == 1 else 0 for x in y_pred_train]
y_pred_test = [1 if x == 1 else 0 for x in y_pred_test]

# Visualizing the results
plt.figure(figsize=(12, 6))

# Plotting training data
plt.subplot(1, 2, 1)
plt.title("Training Data with Anomalies")
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_pred_train, cmap='coolwarm', marker='o', label='Inliers')
plt.scatter(X_test[:, 0], X_test[:, 1], color='black', marker='x', label='Test Points')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

# Plotting test data
plt.subplot(1, 2, 2)
plt.title("Test Data with Anomalies")
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, cmap='coolwarm', marker='o', label='Predictions')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

plt.tight_layout()
plt.show()

"""

custom_palette = sns.color_palette("Set2", n_colors=3)

sns.set_palette(custom_palette)

"""### Univariate Analysis"""

def plot_categorical_univariate(df, categorical_features):
    sns.set(style="darkgrid")
    num_features = len(categorical_features)
    fig, axes = plt.subplots(nrows=(num_features + 1) // 2, ncols=2, figsize=(15, num_features * 3))

    axes = axes.flatten()

    for i, feature in enumerate(categorical_features):
        ax = sns.countplot(data=df, x=feature, ax=axes[i], palette='Pastel1')
        total = len(df)

        for p in ax.patches:
            height = p.get_height()
            percentage = (height / total) * 100
            ax.annotate(f'{percentage:.1f}%',
                        (p.get_x() + p.get_width() / 2, height),
                        ha='center', va='bottom', fontsize=10)

        axes[i].set_title(f'Count of {feature}', fontsize=14)
        axes[i].set_xlabel(feature, fontsize=12)
        axes[i].set_ylabel('Count', fontsize=12)
        axes[i].tick_params(axis='x', rotation=45)

    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

plot_categorical_univariate(df_copy, cat_cols)

"""### Bivariate Analysis"""

def categorical_plot(df, x: str, y:str):
    df[y] = df[y].astype(str)
    df1 = df.groupby(x)[y].value_counts(normalize=True)
    df1 = df1.mul(100)
    df1 = df1.rename('percent').reset_index()

    g = sns.catplot(x=x, y='percent', hue=y, kind='bar', data=df1)

    g.ax.set_ylim(0, 100)

    for p in g.ax.patches:
        txt = str(round(float(p.get_height()), 2)) + '%'
        txt_x = p.get_x() + p.get_width() / 2  # Center the text
        txt_y = p.get_height()
        g.ax.text(txt_x, txt_y, txt, ha='center', va='bottom')  # Align the text
    plt.show()

x, y = 'VisitorType', 'Weekend'
categorical_plot(df, x, y)

months_order = ['Feb', 'Mar', 'May', 'June', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
df['Month'] = pd.Categorical(df['Month'], categories=months_order, ordered=True)

monthly_revenue = df.groupby(['Month', 'Revenue']).size().unstack().fillna(0)
plt.figure(figsize=(10, 6))
monthly_revenue.plot(kind='bar', stacked=False, color=['#FF6F61', '#6BAED6'] , ax=plt.gca())
plt.title('Monthly Revenue Distribution')
plt.xlabel('Month')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.legend(title='Revenue', labels=['No', 'Yes'])  # Adjust labels if needed
plt.show()

"""# Example of bivariate analysis for categorical features
categorical_cols = df_pure.select_dtypes(include= ['category'])
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(x=col, hue='Revenue', data=df)
    plt.title(f'{col} vs Revenue')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.legend(title='Revenue')
    plt.show()

"""



import seaborn as sns
import matplotlib.pyplot as plt

# Histogram with colors
def plot_histogram_vs_revenue(df, column):
    plt.figure(figsize=(4, 4))
    sns.histplot(data=df, x=column, hue='Revenue', multiple='stack', kde=True, palette='Set2')
    plt.title(f'Distribution of {column} by Revenue')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.show()

# Categorical plot with colors
def plot_categorical(df, categorical_column):
    plt.figure(figsize=(8, 6))
    sns.countplot(x=categorical_column, hue='Revenue', data=df, palette='Set2')
    plt.title(f'Count of {categorical_column} by Revenue')
    plt.xlabel(categorical_column)
    plt.ylabel('Count')
    plt.legend(title='Revenue')
    plt.show()

# Boolean plot with colors
def plot_boolean(df, boolean_column):
    plt.figure(figsize=(8, 6))
    sns.countplot(x=boolean_column, hue='Revenue', data=df, palette='Set2')
    plt.title(f'Distribution of {boolean_column} by Revenue')
    plt.xlabel(boolean_column)
    plt.ylabel('Count')
    plt.legend(title='Revenue')
    plt.show()

# Dispatcher function
def plot_by_revenue(df):
    for column in df.columns:
        if column != 'Revenue':
            if df[column].dtype == 'float64' or df[column].dtype == 'int64':
                plot_histogram_vs_revenue(df, column)
            elif df[column].dtype == 'object':
                plot_categorical(df, column)
            elif df[column].dtype == 'bool':
                plot_boolean(df, column)

# Multiple histograms in subplots with color
def plot_histograms_vs_revenue(df, columns):
    fig, axes = plt.subplots(5, 2, figsize=(14, 20))
    fig.tight_layout(pad=5)

    for i, column in enumerate(columns[:10]):
        row, col = divmod(i, 2)
        sns.histplot(data=df, x=column, hue='Revenue', multiple='stack', kde=True, ax=axes[row, col], palette='Set2')
        axes[row, col].set_title(f'Distribution of {column} by Revenue')
        axes[row, col].set_xlabel(column)
        axes[row, col].set_ylabel('Frequency')

    plt.show()

len(num_cols)

plot_histograms_vs_revenue(df_copy, num_cols)

def plot_violinplots_vs_revenue(df, columns):
    fig, axes = plt.subplots(5, 2, figsize=(14, 20))
    fig.tight_layout(pad=5)

    for i, column in enumerate(columns[:10]):
        row, col = divmod(i, 2)

        sns.violinplot(
            data=df,
            x='Revenue',
            y=column,
            ax=axes[row, col],
            inner=None,  # Removes default boxplot inside violin
            palette='Set2'
        )

        # Plot the group means
        means = df.groupby('Revenue')[column].mean()
        for idx, revenue in enumerate(means.index):
            axes[row, col].scatter(x=idx, y=means[revenue], color='red', label='Mean' if idx == 0 else "")

        # Optional: manually add outliers (like in your boxplot)
        for idx, (revenue, group_data) in enumerate(df.groupby('Revenue')[column]):
            q1 = np.percentile(group_data.dropna(), 25)
            q3 = np.percentile(group_data.dropna(), 75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr

            outliers = group_data[(group_data < lower_bound) | (group_data > upper_bound)]
            axes[row, col].scatter([idx] * len(outliers), outliers, color='orange', alpha=0.6, marker='o', label='Outlier' if idx == 0 else "")

        axes[row, col].set_title(f'Violin plot of {column} vs Revenue')
        axes[row, col].set_xlabel('Revenue')
        axes[row, col].set_ylabel(column)

    handles, labels = axes[0, 0].get_legend_handles_labels()
    fig.legend(handles, labels, loc='upper center', ncol=2)
    plt.show()

plot_violinplots_vs_revenue(df_copy, num_cols)

df_traf = pd.crosstab(df['TrafficType'], df['Revenue'])
df_traf.div(df_traf.sum(1).astype(float), axis=0).plot(
    kind='bar',
    stacked=True,
    color=['#FF6F61', '#6BAED6']
)

plt.title('Distribution of the Traffic Type vs Revenue', fontsize=15)
plt.xlabel('Traffic Type', fontsize=8)
plt.ylabel('Proportion', fontsize=8)
plt.xticks(rotation=45)
plt.legend(title='Revenue', fontsize=8)
plt.show()

"""### Multivariate Analysis"""

df_copy.info()

num_cols = df_copy.select_dtypes(include=['number']).columns
num_cols

correlation_matrix = df_copy[num_cols].corr(method='spearman')
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

plt.figure(figsize=(8, 6))

sns.heatmap(correlation_matrix, annot=True, cmap='plasma', mask=mask, annot_kws={"size": 7}, fmt=".2f",
            cbar_kws={'shrink': .8}, linewidths=0.5)

for i in range(len(correlation_matrix)):
    for j in range(len(correlation_matrix)):
        if mask[i, j]:
            plt.text(j + 0.5, i + 0.5, f'{correlation_matrix.iloc[i, j]:.2f}',
                     ha='center', va='center', color='black', fontsize=7)

plt.title('Correlation Heatmap of Numerical Columns')
plt.show()

from sklearn.preprocessing import StandardScaler

# Make a copy to avoid changing original DataFrame
df_normalized = df_copy.copy()

# Select only numerical columns
numerical_cols = df_normalized.select_dtypes(include=['float64', 'int64']).columns

# Normalize
scaler = StandardScaler()
df_normalized[numerical_cols] = scaler.fit_transform(df_normalized[numerical_cols])

# Compute Pearson correlation
correlation_matrix = df_normalized[numerical_cols].corr(method='pearson')

# Plot heatmap
plt.figure(figsize=(10, 8))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

sns.heatmap(
    correlation_matrix,
    mask=mask,
    annot=True,
    cmap='coolwarm',
    fmt=".2f",
    annot_kws={"size": 8},
    cbar_kws={'shrink': 0.8},
    linewidths=0.5,
    square=True,
    center=0
)

plt.title('Normalized Pearson Correlation Heatmap', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""*Bounce Rate* and *Exit Rate* show a strong correlation, which may introduce redundancy into the model. To minimize this, we might remove one feature if its impact is significant. Conducting feature selection can help evaluate their importance and predictive value. Since the correlation is above 0.9, we should test the model’s performance after removing one to confirm that accuracy and interpretability remain intact."""

# month vs bouncerates wrt revenue
custom_colors = ['#4E79A7', '#F28E2B']
sns.boxplot(x=df['Month'], y=df['BounceRates'], hue=df['Revenue'], palette=custom_colors)
plt.title('Month vs Bounce Rates.', fontsize=15)

# Encoding categorical variables
label_encoders = {}
df_label_encoded = df_copy.copy()
for column in df_label_encoded.select_dtypes(include=['category','object', 'bool']).columns:
    le = LabelEncoder()
    df_label_encoded[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Define features and target
X = df_label_encoded.drop(columns='Revenue')
y = df_label_encoded['Revenue']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Model 1: Using both 'Bounce Rate' and 'Exit Rate'
model1 = RandomForestClassifier(random_state=0)
model1.fit(X_train, y_train)
y_pred1 = model1.predict(X_test)
accuracy1 = accuracy_score(y_test, y_pred1)

# Model 2: Removing one of the correlated features ('Exit Rate')
X_train_reduced = X_train.drop(columns=['ExitRates'])
X_test_reduced = X_test.drop(columns=['ExitRates'])

model2 = RandomForestClassifier(random_state=0)
model2.fit(X_train_reduced, y_train)
y_pred2 = model2.predict(X_test_reduced)
accuracy2 = accuracy_score(y_test, y_pred2)

# Model 2: Removing one of the correlated features ('Bounce Rate')
X_train_red = X_train.drop(columns=['BounceRates'])
X_test_red = X_test.drop(columns=['BounceRates'])

model3 = RandomForestClassifier(random_state=0)
model3.fit(X_train_red, y_train)
y_pred3 = model3.predict(X_test_red)
accuracy3 = accuracy_score(y_test, y_pred3)

# Compare the performance
print("Model accuracy with both features:", accuracy1)
print("Model accuracy without 'Exit Rate':", accuracy2)
print("Model accuracy without 'Bounce Rate':", accuracy3)

"""model with both features performed slightly better so we are dropping any one."""

def plot_violinplot(df):
    sns.set(style="whitegrid")

    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))

    sns.violinplot(data=df, x='OperatingSystems', y='Informational', hue='Revenue',
                   ax=axes[0, 0], split=True, palette='Set2', inner="quartile")
    axes[0, 0].set_title('Informational vs Operating Systems by Revenue', fontsize=14)
    axes[0, 0].set_xlabel('Operating Systems', fontsize=12)
    axes[0, 0].set_ylabel('Informational', fontsize=12)

    sns.violinplot(data=df, x='Browser', y='ProductRelated', hue='Revenue',
                   ax=axes[0, 1], split=True, palette='Set2', inner="quartile")
    axes[0, 1].set_title('Product Related vs Browser by Revenue', fontsize=14)
    axes[0, 1].set_xlabel('Browser', fontsize=12)
    axes[0, 1].set_ylabel('Product Related', fontsize=12)

    sns.violinplot(data=df, x='TrafficType', y='PageValues', hue='Revenue',
                   ax=axes[1, 0], split=True, palette='Set2', inner="quartile")
    axes[1, 0].set_title('Page Values vs Traffic Type by Revenue', fontsize=14)
    axes[1, 0].set_xlabel('Traffic Type', fontsize=12)
    axes[1, 0].set_ylabel('Page Values', fontsize=12)

    # Remove empty subplot
    fig.delaxes(axes[1, 1])

    plt.tight_layout()
    axes[0, 1].legend(title='Revenue', loc='upper right', fontsize=10)
    plt.show()

# Call the function
plot_violinplot(df_copy)

sns.violinplot(x=df['Weekend'], y=df['PageValues'], hue=df['Revenue'])



"""# Create a pair plot
sns.set(style='whitegrid')  # Set the style of the seaborn plot
pair_plot = sns.pairplot(df[numerical_columns], diag_kind='kde')  # diag_kind can be 'kde' or 'hist'

# Show the plot
plt.suptitle('Pair Plot of Numerical Columns', y=1.02)  # Adding a title
plt.show()
"""

sns.boxplot(x = df['VisitorType'], y = df['BounceRates'], hue = df['Revenue'], palette = 'Set1')
plt.title('Visitors vs BounceRates w.r.t. Rev.', fontsize=15)

df_copy.isnull().sum().sum()

x = df_copy.iloc[:, [1, 6]].values

# checking the shape of the dataset
x.shape

plt.figure(figsize=(8, 6))
# Calculate WCSS for different cluster counts
wcss = []
for i in range(1, 11):
    km = KMeans(
        n_clusters=i,
        init='k-means++',
        max_iter=300,
        n_init=10,
        random_state=0,
        algorithm='elkan',
        tol=0.001
    )
    km.fit(x)

    # Get the cluster labels
    labels = km.labels_  # corrected to labels_

    # Append the within-cluster sum of squares (WCSS)
    wcss.append(km.inertia_)

# Plotting the elbow method
plt.rcParams['figure.figsize'] = (15, 7)
plt.plot(range(1, 11), wcss)
plt.grid()
plt.tight_layout()
plt.title('The Elbow Method', fontsize=20)
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

x = df_copy.iloc[:, [1, 6]].values

# Apply KMeans clustering
km = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)
y_means = km.fit_predict(x)

# Convert to DataFrame for plotting
df_plot = pd.DataFrame(x, columns=['Administrative_Duration', 'BounceRates'])
df_plot['Cluster'] = y_means

# Map cluster labels to meaningful names
cluster_names = {0: 'Un-interested Customers', 1: 'General Customers', 2: 'Target Customers'}
df_plot['Cluster'] = df_plot['Cluster'].map(cluster_names)

# Define custom color palette for clusters
cluster_palette = {
    'Un-interested Customers': '#E24A33',
    'General Customers': '#348ABD',
    'Target Customers': '#988ED5'
}

# Plot clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=df_plot,
    x='Administrative_Duration',
    y='BounceRates',
    hue='Cluster',
    palette=cluster_palette,
    s=100,
    edgecolor='white'
)

# Plot centroids
centroids = km.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='black', marker='X', label='Centroid')

# Final touches
plt.title('Administrative Duration vs Bounce Rates by Cluster', fontsize=18)
plt.xlabel('Administrative Duration')
plt.ylabel('Bounce Rates')
plt.legend(title='Cluster')
plt.grid(True)
plt.tight_layout()
plt.show()

df_copy.sample(5)

from sklearn.metrics import silhouette_score
import seaborn as sns

# Selecting relevant features for clustering
features = df[['Administrative_Duration','Informational_Duration', 'ProductRelated_Duration',
                'ExitRates', 'PageValues', 'SpecialDay']]

# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Determine the optimal number of clusters using the elbow method
inertia = []
silhouette_scores = []
range_n_clusters = range(2, 11)

for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(features)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(features, kmeans.labels_))

# Plot the elbow method
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(range_n_clusters, inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')

plt.subplot(1, 2, 2)
plt.plot(range_n_clusters, silhouette_scores, marker='o', color='orange')
plt.title('Silhouette Scores for Different k')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()

# Choose the number of clusters based on the elbow method or silhouette score
optimal_clusters = 4  # Adjust based on your analysis
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
kmeans.fit(features)

# Add cluster labels to the original dataframe
df['Cluster'] = kmeans.labels_

# Visualizing the clusters
plt.figure(figsize=(10, 8))
sns.scatterplot(x='Administrative', y='ProductRelated', hue='Cluster',
                data=df, palette='viridis', s=100, alpha=0.7)
plt.title('K-Means Clustering Results')
plt.xlabel('Administrative')
plt.ylabel('Product Related')
plt.legend(title='Cluster')
plt.show()

plt.figure(figsize=(10, 8))
sns.scatterplot(x='ProductRelated', y='BounceRates', hue='Cluster',
                data=df, palette='viridis', s=100, alpha=0.7)
plt.title('K-Means Clustering Results by Customer Category')
plt.xlabel('Product Page')
plt.ylabel('Bounce Rate')
plt.legend(title='Customer Category')
plt.show()

df.info()

"""### Preprocessing"""

df_copy.info()

df_copy.Revenue.value_counts()

df_clean = df_copy.copy()

"""# Experiment Set-up

Preprocessing Steps
* Make a copy of the data
* Label Encodeing - Traffictype, BrowserType and Month are label encoded because high cardinality tendencies
* One hot encoding - the other categorical variable
* Pipeline for Hybrid sampling - SMOTE 60% oversampling and 80% undersampling according to the paper.
* MinMax Scaling numerical varibles because of the fact that most of the columns do not follow normal distribution upon visual inspection of distribution.
* Data Splitting

#### 1. ENCODING, DEFINE TRANSFORMS + SAMPLING
"""

#Label Encoding
le = LabelEncoder()
df_clean['Revenue'] = le.fit_transform(df_clean['Revenue'])
df_clean['TrafficType'] = le.fit_transform(df_clean['TrafficType'])
df_clean['Month'] = le.fit_transform(df_clean['Month'])
df_clean['Browser'] = le.fit_transform(df_clean['Browser'])

df_copy.Revenue.value_counts()

df_clean['Month'].value_counts()

df_clean.info()

# one hot encoding
data1 = pd.get_dummies(df_clean)
data1.columns
#Remember: to Smote

print(df_clean['Revenue'].value_counts())
x=data1
x = x.drop(['Revenue'], axis = 1)
y = data1['Revenue']

print("Shape of x:", x.shape)
print("Shape of y:", y.shape)

numerical_transforms = {
    'standard_scaler': StandardScaler(),
    'minmax_scaler': MinMaxScaler(),
    'robust_scaler': RobustScaler(),
    'power_transform': PowerTransformer(),
    'quantile_transform': QuantileTransformer(output_distribution='normal')
}

sampling_strategies = {
    'smote_0.6_rus_0.8': (SMOTE(sampling_strategy=0.6, random_state=42),
                          RandomUnderSampler(sampling_strategy=0.8, random_state=42))
}

"""#### 2. DEFINE MODEL GRIDS"""

models = {
    'RandomForest': (RandomForestClassifier(random_state=42), {
        'classifier__n_estimators': [100, 200, 300],
        'classifier__max_features': ['auto', 'sqrt'],
        'classifier__max_depth': [None, 10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10],
        'classifier__min_samples_leaf': [1, 2, 4],
        'classifier__bootstrap': [True, False]
    }),
    'XGBoost': (XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), {
        'classifier__n_estimators': [100, 200, 300],
        'classifier__learning_rate': [0.01, 0.1, 0.2],
        'classifier__max_depth': [3, 5, 7],
        'classifier__min_child_weight': [1, 3, 5],
        'classifier__subsample': [0.5, 0.75, 1.0],
        'classifier__colsample_bytree': [0.5, 0.75, 1.0]
    }),
    'MLP': (MLPClassifier(max_iter=200, random_state=42), {
        'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],
        'classifier__activation': ['tanh', 'relu'],
        'classifier__solver': ['sgd', 'adam'],
        'classifier__alpha': [0.0001, 0.001],
        'classifier__learning_rate': ['constant', 'adaptive']
    }),
    'LogisticRegression': (LogisticRegression(random_state=42), {
        'classifier__C': [0.1, 1, 10, 100],
        'classifier__penalty': ['l2'],
        'classifier__solver': ['liblinear']
    })
}



"""#### 3. DATA SPLITTING"""

X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)

"""#### 4. SELECT BEST TRANSFORM USING RANDOM FOREST"""

scaler_results = []

for transform_name, transformer in numerical_transforms.items():
    print(f"Evaluating {transform_name} with Random Forest...")

    preprocessor = ColumnTransformer([
        ('num', transformer, numerical_cols)
    ], remainder='passthrough')

    smote, rus = sampling_strategies['smote_0.6_rus_0.8']

    rf_model, rf_params = models['RandomForest']

    pipeline = ImbPipeline([
        ('preprocessing', preprocessor),
        ('smote', smote),
        ('rus', rus),
        ('classifier', rf_model)
    ])

    search = RandomizedSearchCV(
        pipeline,
        param_distributions=rf_params,
        n_iter=10,
        scoring='f1',
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        n_jobs=-1,
        verbose=0
    )

    search.fit(X_train, y_train)
    y_pred = search.predict(X_test)
    y_proba = search.predict_proba(X_test)[:, 1]

    auc = roc_auc_score(y_test, y_proba)
    f1 = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score']

    scaler_results.append({
        'transform': transform_name,
        'f1_score': f1,
        'roc_auc': auc,
        'best_estimator': search.best_estimator_
    })

"""#### 5. SELECT BEST SCALER"""

results_df = pd.DataFrame(scaler_results)
best_transform = results_df.sort_values(by=['f1_score', 'roc_auc'], ascending=False).iloc[0]
print("\nBest Transform Based on RandomForest:\n", best_transform[['transform', 'f1_score', 'roc_auc']])

selected_transform_name = best_transform['transform']
selected_transform = numerical_transforms[selected_transform_name]

"""#### 6. EVALUATE OTHER MODELS USING BEST TRANSFORM"""

print(f"\nEvaluating all models using the best transform: {selected_transform_name}\n")

final_model_results = []

for model_name, (model, param_grid) in models.items():
    print(f"Tuning {model_name}...")

    preprocessor = ColumnTransformer([
        ('num', selected_transform, numerical_cols)
    ], remainder='passthrough')

    smote, rus = sampling_strategies['smote_0.6_rus_0.8']

    pipeline = ImbPipeline([
        ('preprocessing', preprocessor),
        ('smote', smote),
        ('rus', rus),
        ('classifier', model)
    ])

    search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_grid,
        n_iter=10,
        scoring='f1',
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        n_jobs=-1,
        verbose=0
    )

    search.fit(X_train, y_train)
    y_pred = search.predict(X_test)
    y_proba = search.predict_proba(X_test)[:, 1]

    auc = roc_auc_score(y_test, y_proba)
    f1 = classification_report(y_test, y_pred, output_dict=True)['weighted avg']['f1-score']

    final_model_results.append({
        'model': model_name,
        'f1_score': f1,
        'roc_auc': auc,
        'best_estimator': search.best_estimator_
    })

# Final output
final_results_df = pd.DataFrame(final_model_results).sort_values(by='f1_score', ascending=False)
print("\nFinal Model Comparison:\n", final_results_df[['model', 'f1_score', 'roc_auc']])